{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8be1df1-ae7c-49e9-a341-dbf4734910b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Waste Projection workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cea0c654-74d1-4908-ae14-1f9a71566339",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "#import pandas as pd\n",
    "import sqlite3\n",
    "import utils\n",
    "\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "from utils import calculate_fefo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 'forecast_df' with 61111 lines\n",
      "Loaded 'full_stock_df' with 16191 lines\n",
      "Loaded 'exclusion_df' with 49 lines\n"
     ]
    }
   ],
   "source": [
    "# Specify the database file to delete\n",
    "database_name = 'hf_database.db'\n",
    "\n",
    "# Create a SQLite database and a connection\n",
    "conn = sqlite3.connect(database_name)\n",
    "\n",
    "# ---- Load demand data in a dataframe # forecast_df\n",
    "file_path = '/Users/fil/Documents/my_projects/hf_fefo_waste_projection/datasets/forecast_df_2024_11_20.csv'\n",
    "forecast_df = pd.read_csv(file_path) # Execute the query and load the result into a pandas DataFrame\n",
    "\n",
    "\n",
    "# ---- Load Inventory Data in a dataframe # full_stock_df\n",
    "file_path = '/Users/fil/Documents/my_projects/hf_fefo_waste_projection/datasets/full_stock_df_2024_11_20.csv'\n",
    "full_stock_df = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "# ---- Load Exclusion List in a dataframe # exclusion_df\n",
    "file_path = '/Users/fil/Documents/my_projects/hf_fefo_waste_projection/datasets/exclusion_df_2024_11_20.csv'\n",
    "exclusion_df = pd.read_csv(file_path)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "\n",
    "# Print an output for verification\n",
    "print(\"Loaded 'forecast_df' with \" + str(len(forecast_df)) + \" lines\") # 61.111 lines\n",
    "print(\"Loaded 'full_stock_df' with \" + str(len(full_stock_df)) + \" lines\") # 16.191 lines\n",
    "print(\"Loaded 'exclusion_df' with \" + str(len(exclusion_df)) + \" lines\") # 50 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16191 entries, 0 to 16190\n",
      "Data columns (total 21 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   sku_code                 16191 non-null  object \n",
      " 1   dc_code                  16191 non-null  object \n",
      " 2   tm_id__po_id             16191 non-null  object \n",
      " 3   lot_code                 0 non-null      float64\n",
      " 4   location_id              14290 non-null  object \n",
      " 5   expiration_date          16191 non-null  object \n",
      " 6   quantity                 16191 non-null  float64\n",
      " 7   category                 16191 non-null  object \n",
      " 8   unit_cost                16191 non-null  float64\n",
      " 9   discardment_date         16191 non-null  object \n",
      " 10  opening_stock__arr_date  16191 non-null  object \n",
      " 11  snapshot_time            14290 non-null  object \n",
      " 12  product_types            16142 non-null  object \n",
      " 13  hellofresh_week          16079 non-null  object \n",
      " 14  hf_week_out              16191 non-null  int64  \n",
      " 15  temperature_class        16191 non-null  object \n",
      " 16  data_source              16191 non-null  object \n",
      " 17  logical_mlor             1901 non-null   float64\n",
      " 18  supplier_code            1901 non-null   object \n",
      " 19  ssku_mlor                1787 non-null   float64\n",
      " 20  mlor_source              1901 non-null   object \n",
      "dtypes: float64(5), int64(1), object(15)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "full_stock_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ac9fb360-2dd7-4634-9c4a-9c9e61e37168",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean Inventory Data by Filtering Out the Eclusion List | # merged = full stock | # cleaned = stock\n",
    "\n",
    "# Merge the inventory list and the exclusion list\n",
    "merged_inventory_df = full_stock_df.merge(exclusion_df, on=['sku_code', 'supplier_code', 'data_source'], how='left', indicator=True)\n",
    "\n",
    "# Filter out: from the _merge column keep only values that do not appear in the right (exclusion) table\n",
    "stock_df = merged_inventory_df[merged_inventory_df['_merge'] == 'left_only'].drop(columns='_merge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e24cc8ce-6643-4a0c-a0bb-2577e5ddd771",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Run the Calculation of FEFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "874db3a0-88f7-453e-9ae4-e86b04c568cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation complete: 16186 lines in the output\n"
     ]
    }
   ],
   "source": [
    "# Calculate FEFO\n",
    "calc_df = calculate_fefo(forecast_df, stock_df)\n",
    "\n",
    "# Print an output for verification\n",
    "print(\"Calculation complete: \" + str(len(calc_df)) + \" lines in the output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16186 entries, 0 to 16185\n",
      "Data columns (total 21 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   sku_id            16186 non-null  object        \n",
      " 1   batch_id          0 non-null      float64       \n",
      " 2   pallet_id         16186 non-null  object        \n",
      " 3   expiration_date   16186 non-null  datetime64[ns]\n",
      " 4   discardment_date  16186 non-null  datetime64[ns]\n",
      " 5   remaining_qty     16186 non-null  float64       \n",
      " 6   consumed_qty      16186 non-null  float64       \n",
      " 7   dc                16186 non-null  object        \n",
      " 8   location          14289 non-null  object        \n",
      " 9   category          16186 non-null  object        \n",
      " 10  unit_cost         16186 non-null  float64       \n",
      " 11  line_cost         16186 non-null  float64       \n",
      " 12  type              16137 non-null  object        \n",
      " 13  hf_week           16074 non-null  object        \n",
      " 14  hf_week_out       16186 non-null  int64         \n",
      " 15  temp_class        16186 non-null  object        \n",
      " 16  data_source       16186 non-null  object        \n",
      " 17  logical_mlor      1897 non-null   float64       \n",
      " 18  mlor_source       1897 non-null   object        \n",
      " 19  snapshot_time     14289 non-null  datetime64[ns]\n",
      " 20  supplier_code     1897 non-null   object        \n",
      "dtypes: datetime64[ns](3), float64(6), int64(1), object(11)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "calc_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sku_id</th>\n",
       "      <th>batch_id</th>\n",
       "      <th>pallet_id</th>\n",
       "      <th>expiration_date</th>\n",
       "      <th>discardment_date</th>\n",
       "      <th>remaining_qty</th>\n",
       "      <th>consumed_qty</th>\n",
       "      <th>dc</th>\n",
       "      <th>location</th>\n",
       "      <th>category</th>\n",
       "      <th>...</th>\n",
       "      <th>line_cost</th>\n",
       "      <th>type</th>\n",
       "      <th>hf_week</th>\n",
       "      <th>hf_week_out</th>\n",
       "      <th>temp_class</th>\n",
       "      <th>data_source</th>\n",
       "      <th>logical_mlor</th>\n",
       "      <th>mlor_source</th>\n",
       "      <th>snapshot_time</th>\n",
       "      <th>supplier_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C_1-10344</td>\n",
       "      <td>NaN</td>\n",
       "      <td>id_493476</td>\n",
       "      <td>2025-02-24</td>\n",
       "      <td>2025-02-20</td>\n",
       "      <td>3213.0</td>\n",
       "      <td>6787.0</td>\n",
       "      <td>FI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C_1</td>\n",
       "      <td>...</td>\n",
       "      <td>3148.74</td>\n",
       "      <td>Ingredient SKU</td>\n",
       "      <td>2025-W08</td>\n",
       "      <td>3</td>\n",
       "      <td>a1</td>\n",
       "      <td>po</td>\n",
       "      <td>84.0</td>\n",
       "      <td>fixed_value_MLOR</td>\n",
       "      <td>NaT</td>\n",
       "      <td>s_5230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C_1-10344</td>\n",
       "      <td>NaN</td>\n",
       "      <td>id_211571</td>\n",
       "      <td>2025-05-02</td>\n",
       "      <td>2025-04-28</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>FI</td>\n",
       "      <td>loc-2115</td>\n",
       "      <td>C_1</td>\n",
       "      <td>...</td>\n",
       "      <td>196.00</td>\n",
       "      <td>Ingredient SKU</td>\n",
       "      <td>2025-W18</td>\n",
       "      <td>3</td>\n",
       "      <td>a1</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-11-19 23:45:04.657</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C_1-10344</td>\n",
       "      <td>NaN</td>\n",
       "      <td>id_211648</td>\n",
       "      <td>2025-05-02</td>\n",
       "      <td>2025-04-28</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>FI</td>\n",
       "      <td>loc-1818</td>\n",
       "      <td>C_1</td>\n",
       "      <td>...</td>\n",
       "      <td>1176.00</td>\n",
       "      <td>Ingredient SKU</td>\n",
       "      <td>2025-W18</td>\n",
       "      <td>3</td>\n",
       "      <td>a1</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-11-19 23:45:04.657</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C_1-10344</td>\n",
       "      <td>NaN</td>\n",
       "      <td>id_871980</td>\n",
       "      <td>2025-05-02</td>\n",
       "      <td>2025-04-28</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>FI</td>\n",
       "      <td>loc-6725</td>\n",
       "      <td>C_1</td>\n",
       "      <td>...</td>\n",
       "      <td>4.90</td>\n",
       "      <td>Ingredient SKU</td>\n",
       "      <td>2025-W18</td>\n",
       "      <td>3</td>\n",
       "      <td>a1</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-11-19 23:45:04.657</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C_1-10344</td>\n",
       "      <td>NaN</td>\n",
       "      <td>id_915969</td>\n",
       "      <td>2025-05-02</td>\n",
       "      <td>2025-04-28</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>FI</td>\n",
       "      <td>loc-6769</td>\n",
       "      <td>C_1</td>\n",
       "      <td>...</td>\n",
       "      <td>7.84</td>\n",
       "      <td>Ingredient SKU</td>\n",
       "      <td>2025-W18</td>\n",
       "      <td>3</td>\n",
       "      <td>a1</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-11-19 23:45:04.657</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sku_id  batch_id  pallet_id expiration_date discardment_date  \\\n",
       "0  C_1-10344       NaN  id_493476      2025-02-24       2025-02-20   \n",
       "1  C_1-10344       NaN  id_211571      2025-05-02       2025-04-28   \n",
       "2  C_1-10344       NaN  id_211648      2025-05-02       2025-04-28   \n",
       "3  C_1-10344       NaN  id_871980      2025-05-02       2025-04-28   \n",
       "4  C_1-10344       NaN  id_915969      2025-05-02       2025-04-28   \n",
       "\n",
       "   remaining_qty  consumed_qty  dc  location category  ...  line_cost  \\\n",
       "0         3213.0        6787.0  FI       NaN      C_1  ...    3148.74   \n",
       "1          200.0           0.0  FI  loc-2115      C_1  ...     196.00   \n",
       "2         1200.0           0.0  FI  loc-1818      C_1  ...    1176.00   \n",
       "3            5.0           0.0  FI  loc-6725      C_1  ...       4.90   \n",
       "4            8.0           0.0  FI  loc-6769      C_1  ...       7.84   \n",
       "\n",
       "             type   hf_week hf_week_out  temp_class data_source logical_mlor  \\\n",
       "0  Ingredient SKU  2025-W08           3          a1          po         84.0   \n",
       "1  Ingredient SKU  2025-W18           3          a1          in          NaN   \n",
       "2  Ingredient SKU  2025-W18           3          a1          in          NaN   \n",
       "3  Ingredient SKU  2025-W18           3          a1          in          NaN   \n",
       "4  Ingredient SKU  2025-W18           3          a1          in          NaN   \n",
       "\n",
       "        mlor_source           snapshot_time supplier_code  \n",
       "0  fixed_value_MLOR                     NaT        s_5230  \n",
       "1               NaN 2024-11-19 23:45:04.657           NaN  \n",
       "2               NaN 2024-11-19 23:45:04.657           NaN  \n",
       "3               NaN 2024-11-19 23:45:04.657           NaN  \n",
       "4               NaN 2024-11-19 23:45:04.657           NaN  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edd09c67-7ae7-4cab-b671-e22971ce6f14",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "04d288de-3b42-492e-bf00-2f22d85060c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n            ### Table that shows Top 5 SKUs by Cost by Category and by Window\\n\\n\\n# Filter the DataFrame for the specific HF Week values\\nwindow_2 = calc_df[calc_df['hf_week_out'].isin(window_2)]\\n\\n# Group by SKU ID, DC, and Category, then calculate the sum of Line Cost for each group\\nranked_cat_window = window_2.groupby(['sku_id', 'dc', 'category'])['line_cost'].sum().reset_index()\\n\\n# Rank the Line Cost within each DC and Category group in descending order\\nranked_cat_window['rank'] = ranked_cat_window.groupby(['dc', 'category'])['line_cost'].rank(ascending=False, method='dense')\\n\\n# Filter to keep only the top 5 SKUs by cost within each Category and DC\\ntop_5_ranked_cat_window = ranked_cat_window[ranked_cat_window['rank'] <= 5]\\n\\n# Merge the top SKUs back with the original DataFrame to include SKU names and other details\\ntop_5_ranked_cat_window = pd.merge(top_5_ranked_cat_window, window_2[['sku_id', 'HF Week Out Name']], on='sku_id').drop_duplicates()\\n\\n# Select only the required columns: DC, Category, SKU ID, SKU Name, Line Cost, and ranking\\ntop_5_ranked_cat_window = top_5_ranked_cat_window[['dc', 'category', 'HF Week Out Name', 'rank', 'sku_id', 'line_cost']]\\n\\n# Sort the final DataFrame by DC, Category, and rank\\ntop_5_ranked_cat_window = top_5_ranked_cat_window.sort_values(by=['dc', 'category', 'rank'])\\n\\n# Display the DataFrame\\ntop_5_ranked_cat_window\\n\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define the weeks you want to filter for\n",
    "window_1_3 = [1, 2, 3]  # Weeks 1-24\n",
    "window_1_2 = [1, 2]  # Weeks 1-12\n",
    "window_2 = [2]  # Weeks 7-12\n",
    "\n",
    "# Define the mapping from HF Week Out to the desired name\n",
    "hf_week_out_mapping = {\n",
    "    0: 'W <0',\n",
    "    1: 'W 01-06',\n",
    "    2: 'W 07-12',\n",
    "    3: 'W 13-24',\n",
    "    4: 'W > 24'\n",
    "}\n",
    "\n",
    "# Map the HF Week Out to the desired names\n",
    "calc_df['HF Week Out Name'] = calc_df['hf_week_out'].map(hf_week_out_mapping)\n",
    "\n",
    "\n",
    "\n",
    "            ### Table that shows aggregate sum of Cost by Category and Window\n",
    "# Filter the DataFrame for the specific HF Week values\n",
    "window_1_3_df = calc_df[calc_df['hf_week_out'].isin(window_1_3)]\n",
    "\n",
    "# Pivot the table to have DC as columns, aggregating the sum of Line Cost\n",
    "category_agg_window_df = window_1_3_df.pivot_table(index=['category', 'HF Week Out Name'], columns='dc', values='line_cost', aggfunc='sum', fill_value=0).reset_index()\n",
    "\n",
    "# Sort the pivoted DataFrame by 'Category' and 'HF Week Out Name'\n",
    "category_agg_window_df = category_agg_window_df.sort_values(by=['HF Week Out Name', 'category'])\n",
    "\n",
    "\n",
    "\n",
    "            ### Table that shows aggregate sum of Cost by Category, weekly\n",
    "# Filter the DataFrame for the specific HF Week values\n",
    "window_1_2_df = calc_df[calc_df['hf_week_out'].isin(window_1_2)]\n",
    "\n",
    "# Pivot the table to have DC as columns, aggregating the sum of Line Cost\n",
    "category_agg_weekly_df = window_1_2_df.pivot_table(index=['dc', 'category'], columns='hf_week', values='line_cost', aggfunc='sum', fill_value=0).reset_index()\n",
    "\n",
    "# Sort the pivoted DataFrame by 'Category' and 'HF Week Out Name'\n",
    "category_agg_weekly_df = category_agg_weekly_df.sort_values(by=['dc', 'category'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca76bc8c-32e3-444c-8c20-2b9bef03edb7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## create Top dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc3df9c1-4791-4f42-bcb5-2a1bb05cf187",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                SKU ID  DC   HF Week  Line Cost           Time Stamp\n",
      "6     BAK-00-002995-5  VE  2025-W35   5581.800  2024-10-03 19:29:24\n",
      "7     BAK-00-002996-5  BX  2025-W37   4622.700  2024-10-03 19:29:24\n",
      "9     BAK-00-002996-5  VE  2024-W40   7914.450  2024-10-03 19:29:24\n",
      "26    BAK-00-003853-5  VE  2025-W07   3833.364  2024-10-03 19:29:24\n",
      "33    BAK-00-003854-5  VE  2025-W08   3529.440  2024-10-03 19:29:24\n",
      "...               ...  ..       ...        ...                  ...\n",
      "3437   SPI-00-90517-5  VE  2025-W45   4768.160  2024-10-03 19:29:24\n",
      "3459   SPI-00-90523-5  BX  2025-W26   3605.760  2024-10-03 19:29:24\n",
      "3465  SPI-11-119380-5  VE  2025-W09   3091.200  2024-10-03 19:29:24\n",
      "3466  SPI-11-119380-5  VE  2025-W13   3709.440  2024-10-03 19:29:24\n",
      "3468  SPI-11-134623-5  BX  2025-W14   4022.370  2024-10-03 19:29:24\n",
      "\n",
      "[188 rows x 5 columns]>\n"
     ]
    }
   ],
   "source": [
    "# Grouping the data and aggregating\n",
    "aggregated_data = calc_df_gsheet\\\n",
    "    .groupby(['SKU ID', 'DC', 'HF Week'])\\\n",
    "    .agg({'Line Cost': 'sum'})\\\n",
    "    .reset_index()\n",
    "\n",
    "# Add 'Time Stamp' column to aggregated_data\n",
    "aggregated_data['Time Stamp'] = current_time_berlin\n",
    "\n",
    "# Filtering the aggregated results\n",
    "top_waste = aggregated_data[aggregated_data['Line Cost'] > 3000]\n",
    "\n",
    "print(top_waste.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b1e030f-61f8-41d6-ab83-3d321f2615fb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Write to databricks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1306326-6d43-4c1d-b9d7-16e0e8e45df4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### append a ts for identification of the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90ce2fbb-9565-47dc-81c0-b6bfedbda4fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-10-03 19:29:24'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a timestamp column in local timezone\n",
    "berlin_tz = pytz.timezone('Europe/Berlin')\n",
    "\n",
    "# Get the current time in Berlin\n",
    "current_time_berlin = datetime.now(berlin_tz).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "current_time_berlin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88a9997a-72d8-40eb-9212-e48799116c24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# If current_time_berlin is a string, convert it to a timestamp\n",
    "calc_df['ts'] = pd.to_datetime(current_time_berlin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9602963-4092-4834-8ba8-bebe4d974844",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### convert to spark and push to databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b747400-faef-4615-892e-45c5269d651b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "calc_spark = spark.createDataFrame(calc_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9466d166-a4a1-4ef5-ae38-f726e032a39d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "convert timestampts to strings and blank mlors to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b75afc31-0231-4b44-8c4b-46664307d708",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, date_format\n",
    "calc_spark = (calc_spark\n",
    "  .withColumn(\"expiration_date\", date_format(\"expiration_date\", \"yyyy-MM-dd\"))\n",
    "  .withColumn(\"discardment_date\", date_format(\"discardment_date\", \"yyyy-MM-dd\"))\n",
    "  .withColumn(\"logical_mlor\", col(\"logical_mlor\").cast(\"double\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d116da8d-3ddf-4b73-aa9b-eebf032875ee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "convert blanks to nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "051e089c-0354-4def-9891-bc62a7a81399",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Step 1: Identify all string columns in the DataFrame\n",
    "string_columns = [col_name for col_name, dtype in calc_spark.dtypes if dtype == \"string\"]\n",
    "\n",
    "# Step 2: Replace empty strings with null for all string columns\n",
    "for col_name in string_columns:\n",
    "    calc_spark = calc_spark.withColumn(col_name, when(col(col_name) == \"\", None).otherwise(col(col_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8346afe5-85cf-40f0-b628-9e00dd421ec4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "calc_spark.write.mode(\"append\").saveAsTable(\"views_analysts.fefo_waste_projection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d9b6517-09a4-4d18-9844-90c6b8e0aab6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Push data to file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34e70bcf-d9c6-4806-bef8-e1ca9a728ea3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Push Calculation Dataframe to Gsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "fef25992-8e65-4e17-a155-95eecbd88d92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n",
      "/root/.ipykernel/1314/command-2040290817187298-1332147831:44: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
      "  worksheet.update('A1', data)  # Update with new data starting from cell A1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEFO Calculation data successfully written to Google Sheet.\n"
     ]
    }
   ],
   "source": [
    "# Save the entire DataFrame to a Google Sheet\n",
    "spreadsheet = client.open(\"FEFO Inventoy Management\")\n",
    "worksheet = spreadsheet.worksheet('Calculation')\n",
    "\n",
    "# Convert datetime.date columns to strings for Google Sheets\n",
    "calc_df['expiration_date'] = calc_df['expiration_date'].astype(str)\n",
    "calc_df['discardment_date'] = calc_df['discardment_date'].astype(str)\n",
    "calc_df['snapshot_time'] = calc_df['snapshot_time'].astype(str)\n",
    "calc_df['pallet_id'] = calc_df['pallet_id'].astype(str)\n",
    "calc_df['ts'] = calc_df['ts'].astype(str)\n",
    "\n",
    "\n",
    "# Modify the column names to make them readable\n",
    "calc_df_gsheet = calc_df.rename(columns={\n",
    "    'sku_id': 'SKU ID',\n",
    "    'batch_id': 'Batch ID',\n",
    "    'pallet_id': 'ID',\n",
    "    'expiration_date': 'Expiration Date',\n",
    "    'discardment_date': 'Discardment Date',\n",
    "    'remaining_qty': 'Remaining Qty',\n",
    "    'consumed_qty': 'Consumed Qty',\n",
    "    'dc': 'DC',\n",
    "    'name': 'Name',\n",
    "    'location': 'Location',\n",
    "    'category': 'Category',\n",
    "    'unit_cost': 'Unit Cost',\n",
    "    'line_cost': 'Line Cost',\n",
    "    'type': 'Type',\n",
    "    'hf_week': 'HF Week',\n",
    "    'hf_week_out': 'HF Week Out',\n",
    "    'temp_class': 'Temperature Class',\n",
    "    'data_source': 'Data Source',\n",
    "    'logical_mlor': 'Logical MLOR',\n",
    "    'mlor_source': 'MLOR source',\n",
    "    'snapshot_time': 'Snapshot Time',\n",
    "    'supplier_code': 'Supplier Code'\n",
    "})\n",
    "\n",
    "# Convert the DataFrame to a list of lists\n",
    "data = [calc_df_gsheet.columns.values.tolist()] + calc_df_gsheet.values.tolist()\n",
    "\n",
    "# Write the entire DataFrame to the Google Sheet\n",
    "worksheet.clear()  # Clear existing data\n",
    "worksheet.update('A1', data)  # Update with new data starting from cell A1\n",
    "\n",
    "print(\"FEFO Calculation data successfully written to Google Sheet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57a5fc8f-49a8-4f83-b8cf-bd2acbd04bd7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Log script end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9cff9c86-f49a-4d03-9a26-ac3b3085bdc6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'spreadsheetId': '1K96XSVvJmy2qpk0tG7gjrFrKlC-0ygpgyy2dQglqZBA',\n",
       " 'updatedRange': 'log!A164',\n",
       " 'updatedRows': 1,\n",
       " 'updatedColumns': 1,\n",
       " 'updatedCells': 1}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select \"Sheet4\" within the Google Sheet\n",
    "worksheet = spreadsheet.worksheet(\"log\")\n",
    "\n",
    "# Create the message to write\n",
    "message = f\"Job FEFO WASTE CALCULATION executed succesfully at {current_time_berlin}\"\n",
    "\n",
    "# Find the next empty row\n",
    "next_empty_row = len(worksheet.col_values(1)) + 1\n",
    "\n",
    "# Write the message in the next empty row\n",
    "worksheet.update_cell(next_empty_row, 1, message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edd09c67-7ae7-4cab-b671-e22971ce6f14",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "04d288de-3b42-492e-bf00-2f22d85060c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n            ### Table that shows Top 5 SKUs by Cost by Category and by Window\\n\\n\\n# Filter the DataFrame for the specific HF Week values\\nwindow_2 = calc_df[calc_df['hf_week_out'].isin(window_2)]\\n\\n# Group by SKU ID, DC, and Category, then calculate the sum of Line Cost for each group\\nranked_cat_window = window_2.groupby(['sku_id', 'dc', 'category'])['line_cost'].sum().reset_index()\\n\\n# Rank the Line Cost within each DC and Category group in descending order\\nranked_cat_window['rank'] = ranked_cat_window.groupby(['dc', 'category'])['line_cost'].rank(ascending=False, method='dense')\\n\\n# Filter to keep only the top 5 SKUs by cost within each Category and DC\\ntop_5_ranked_cat_window = ranked_cat_window[ranked_cat_window['rank'] <= 5]\\n\\n# Merge the top SKUs back with the original DataFrame to include SKU names and other details\\ntop_5_ranked_cat_window = pd.merge(top_5_ranked_cat_window, window_2[['sku_id', 'HF Week Out Name']], on='sku_id').drop_duplicates()\\n\\n# Select only the required columns: DC, Category, SKU ID, SKU Name, Line Cost, and ranking\\ntop_5_ranked_cat_window = top_5_ranked_cat_window[['dc', 'category', 'HF Week Out Name', 'rank', 'sku_id', 'line_cost']]\\n\\n# Sort the final DataFrame by DC, Category, and rank\\ntop_5_ranked_cat_window = top_5_ranked_cat_window.sort_values(by=['dc', 'category', 'rank'])\\n\\n# Display the DataFrame\\ntop_5_ranked_cat_window\\n\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define the weeks you want to filter for\n",
    "window_1_3 = [1, 2, 3]  # Weeks 1-24\n",
    "window_1_2 = [1, 2]  # Weeks 1-12\n",
    "window_2 = [2]  # Weeks 7-12\n",
    "\n",
    "# Define the mapping from HF Week Out to the desired name\n",
    "hf_week_out_mapping = {\n",
    "    0: 'W <0',\n",
    "    1: 'W 01-06',\n",
    "    2: 'W 07-12',\n",
    "    3: 'W 13-24',\n",
    "    4: 'W > 24'\n",
    "}\n",
    "\n",
    "# Map the HF Week Out to the desired names\n",
    "calc_df['HF Week Out Name'] = calc_df['hf_week_out'].map(hf_week_out_mapping)\n",
    "\n",
    "\n",
    "\n",
    "            ### Table that shows aggregate sum of Cost by Category and Window\n",
    "# Filter the DataFrame for the specific HF Week values\n",
    "window_1_3_df = calc_df[calc_df['hf_week_out'].isin(window_1_3)]\n",
    "\n",
    "# Pivot the table to have DC as columns, aggregating the sum of Line Cost\n",
    "category_agg_window_df = window_1_3_df.pivot_table(index=['category', 'HF Week Out Name'], columns='dc', values='line_cost', aggfunc='sum', fill_value=0).reset_index()\n",
    "\n",
    "# Sort the pivoted DataFrame by 'Category' and 'HF Week Out Name'\n",
    "category_agg_window_df = category_agg_window_df.sort_values(by=['HF Week Out Name', 'category'])\n",
    "\n",
    "\n",
    "\n",
    "            ### Table that shows aggregate sum of Cost by Category, weekly\n",
    "# Filter the DataFrame for the specific HF Week values\n",
    "window_1_2_df = calc_df[calc_df['hf_week_out'].isin(window_1_2)]\n",
    "\n",
    "# Pivot the table to have DC as columns, aggregating the sum of Line Cost\n",
    "category_agg_weekly_df = window_1_2_df.pivot_table(index=['dc', 'category'], columns='hf_week', values='line_cost', aggfunc='sum', fill_value=0).reset_index()\n",
    "\n",
    "# Sort the pivoted DataFrame by 'Category' and 'HF Week Out Name'\n",
    "category_agg_weekly_df = category_agg_weekly_df.sort_values(by=['dc', 'category'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02521f22-7019-4d41-a036-8b74770bb8fa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Push aggregations data to sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "033b1eb5-cd51-4cbd-bc37-49f5d481fe43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'spreadsheetId': '1K96XSVvJmy2qpk0tG7gjrFrKlC-0ygpgyy2dQglqZBA',\n",
       " 'updatedRange': 'Overview!A31:N47',\n",
       " 'updatedRows': 17,\n",
       " 'updatedColumns': 14,\n",
       " 'updatedCells': 238}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the entire DataFrame to a Google Sheet\n",
    "spreadsheet = client.open(\"FEFO Inventoy Management\")\n",
    "worksheet = spreadsheet.worksheet('Overview')\n",
    "\n",
    "\n",
    "# Convert the DataFrame to a list of lists\n",
    "data_1 = [category_agg_window_df.columns.values.tolist()] + category_agg_window_df.values.tolist()\n",
    "\n",
    "# Write the entire DataFrame to the Google Sheet\n",
    "worksheet.clear()  # Clear existing data\n",
    "worksheet.update(range_name='A1', values=data_1)  # Update with new data starting from cell A1\n",
    "\n",
    "\n",
    "\n",
    "# \n",
    "# Convert the DataFrame to a list of lists\n",
    "data_2 = [category_agg_weekly_df.columns.values.tolist()] + category_agg_weekly_df.values.tolist()\n",
    "\n",
    "# Write the entire DataFrame to the Google Sheet\n",
    "worksheet.update(range_name='A31', values=data_2)  # Update with new data starting from cell A1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dcdfaae-71ed-40a6-bc78-0dd34141d918",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Push aggregation data to PROCUREMENT PERFORMANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d4eb1c14-67c9-4374-a01a-184c0626c081",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    }
   ],
   "source": [
    "# Introduce a sleep time to avoid hitting API limits\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "28199756-984b-4ba8-9a61-ed1bf4db1ed5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n",
      "/root/.ipykernel/1314/command-9196251967406-4068024711:14: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
      "  worksheet.update('A1', [[message]])\n",
      "/root/.ipykernel/1314/command-9196251967406-4068024711:15: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
      "  worksheet.update('E2', [[\"Online Menu W:01-06\"]])\n",
      "/root/.ipykernel/1314/command-9196251967406-4068024711:16: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
      "  worksheet.update('K2', [[\"Offline Menu W:07-12\"]])\n",
      "INFO:root:Operation succeeded on attempt 1\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "# Set up logging (optional)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "retries = 3\n",
    "for attempt in range(retries):\n",
    "    try:\n",
    "        # Perform your Google Sheets operation\n",
    "        spreadsheet = client.open(\"Procurement Performance\")\n",
    "        worksheet = spreadsheet.worksheet('waste_projection_pivots')\n",
    "        worksheet.clear()\n",
    "        worksheet.update(range_name='C4', values=data_2) \n",
    "        worksheet.update('A1', [[message]])\n",
    "        worksheet.update('E2', [[\"Online Menu W:01-06\"]])\n",
    "        worksheet.update('K2', [[\"Offline Menu W:07-12\"]])\n",
    "\n",
    "        # Log the successful attempt\n",
    "        logging.info(f\"Operation succeeded on attempt {attempt + 1}\")\n",
    "        # Optionally, you can print instead of logging:\n",
    "        # print(f\"Operation succeeded on attempt {attempt + 1}\")\n",
    "        \n",
    "        break  # Exit loop on success\n",
    "    except APIError as e:\n",
    "        if attempt < retries - 1:\n",
    "            logging.warning(f\"Attempt {attempt + 1} failed, retrying...\")\n",
    "            time.sleep(2)  # Add delay before retrying\n",
    "        else:\n",
    "            logging.error(\"All attempts failed.\")\n",
    "            raise e  # If last attempt fails, raise the exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca76bc8c-32e3-444c-8c20-2b9bef03edb7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## create Top dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc3df9c1-4791-4f42-bcb5-2a1bb05cf187",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                SKU ID  DC   HF Week  Line Cost           Time Stamp\n",
      "6     BAK-00-002995-5  VE  2025-W35   5581.800  2024-10-03 19:29:24\n",
      "7     BAK-00-002996-5  BX  2025-W37   4622.700  2024-10-03 19:29:24\n",
      "9     BAK-00-002996-5  VE  2024-W40   7914.450  2024-10-03 19:29:24\n",
      "26    BAK-00-003853-5  VE  2025-W07   3833.364  2024-10-03 19:29:24\n",
      "33    BAK-00-003854-5  VE  2025-W08   3529.440  2024-10-03 19:29:24\n",
      "...               ...  ..       ...        ...                  ...\n",
      "3437   SPI-00-90517-5  VE  2025-W45   4768.160  2024-10-03 19:29:24\n",
      "3459   SPI-00-90523-5  BX  2025-W26   3605.760  2024-10-03 19:29:24\n",
      "3465  SPI-11-119380-5  VE  2025-W09   3091.200  2024-10-03 19:29:24\n",
      "3466  SPI-11-119380-5  VE  2025-W13   3709.440  2024-10-03 19:29:24\n",
      "3468  SPI-11-134623-5  BX  2025-W14   4022.370  2024-10-03 19:29:24\n",
      "\n",
      "[188 rows x 5 columns]>\n"
     ]
    }
   ],
   "source": [
    "# Grouping the data and aggregating\n",
    "aggregated_data = calc_df_gsheet\\\n",
    "    .groupby(['SKU ID', 'DC', 'HF Week'])\\\n",
    "    .agg({'Line Cost': 'sum'})\\\n",
    "    .reset_index()\n",
    "\n",
    "# Add 'Time Stamp' column to aggregated_data\n",
    "aggregated_data['Time Stamp'] = current_time_berlin\n",
    "\n",
    "# Filtering the aggregated results\n",
    "top_waste = aggregated_data[aggregated_data['Line Cost'] > 3000]\n",
    "\n",
    "print(top_waste.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2a4f258-3484-4482-b40c-588f576f7065",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### load the top to a temporary storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dbd609c-78da-4237-b642-cb3e685e2308",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    }
   ],
   "source": [
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "top_waste_spark = spark.createDataFrame(top_waste)\n",
    "\n",
    "# Now write the Spark DataFrame to disk\n",
    "top_waste_spark.write.format(\"parquet\").mode(\"overwrite\").save(\"/dbfs/tmp/top_waste.parquet\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "fefo_waste_projection_workflow",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
